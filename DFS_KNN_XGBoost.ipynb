{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feature Synthesis - PCA - KNN and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Score on Driven Data KNN: 0.6995\n",
    "Score on Driven Data XGBoost: 0.7279\n",
    "\n",
    "#**Summary:** For these submissions our goal was to learn a little bit about Deep Feature Synthesis for automated feature generation and see how these new features impacted our score. We must say the attempt was not as successfull as others, first because the score on Driven Data was much lower but mostly because the gap between the cross-validation scores where higher than the scores we got on Driven Data. This might be a matter of the way the DFS was implemented : because of time and computational power restrictions we decided to perform PCA after the feature generation to be able to fit somehow the outcome of DFS in our model but these did not seem to capture the data better than the original features. All and all it was an interesting experience for us to play a little bit with the different aggregation methos that DFS provides and hope to get a better understandment in future works, as we have found from experience that feature creation (at least manually) can have a very positive impact on prediction.\n",
    "\n",
    "**Content:**\n",
    "1. Data Loading\n",
    "2. Data Cleaning\n",
    "3. One hote encoding and scaling\n",
    "4. Feature Creation\n",
    " 4.1 K-means clustering for location\n",
    " 4.2 Deep Feature Synthesis\n",
    "5. PCA\n",
    "6. Models\n",
    " 6.1 KNN\n",
    " 6.2 XGboost\n",
    "7.Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:46.049517Z",
     "start_time": "2020-03-17T22:46:41.775376Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "import featuretools as ft\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:47.838846Z",
     "start_time": "2020-03-17T22:46:46.053429Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "X_train = pd.read_csv('train_features.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')\n",
    "X_test = pd.read_csv('test_features.csv')\n",
    "y_test = pd.read_csv('submission_format.csv')\n",
    "\n",
    "# merge features and labels on train set\n",
    "train = X_train.copy()\n",
    "train = train.merge(y_train, how = 'left', on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:47.868352Z",
     "start_time": "2020-03-17T22:46:47.844352Z"
    }
   },
   "outputs": [],
   "source": [
    "# column to always drop\n",
    "columns_to_drop = [\n",
    "\n",
    "    'subvillage',\n",
    "    'region_code',\n",
    "    'district_code',\n",
    "    'wpt_name',\n",
    "    'recorded_by',\n",
    "    'scheme_name',\n",
    "    'management_group',\n",
    "    'payment',\n",
    "    'extraction_type_group',\n",
    "    'extraction_type_class',\n",
    "    'waterpoint_type_group',\n",
    "    'quality_group',\n",
    "    'quantity_group',\n",
    "    'source_type',\n",
    "    'source_class',\n",
    "    'num_private', \n",
    "    'date_recorded',\n",
    "  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:47.902300Z",
     "start_time": "2020-03-17T22:46:47.889771Z"
    }
   },
   "outputs": [],
   "source": [
    "# columns to drop for now\n",
    "additional_columns_to_drop = [\n",
    "    'funder',\n",
    "    'installer',\n",
    "    'amount_tsh',\n",
    "    'lga',\n",
    "    'ward',\n",
    "    'scheme_management'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:48.021440Z",
     "start_time": "2020-03-17T22:46:47.909801Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "X_train.drop(additional_columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "X_test.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "X_test.drop(additional_columns_to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning\n",
    "\n",
    "Following the same approach as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:48.104464Z",
     "start_time": "2020-03-17T22:46:48.026574Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a column storing the info whether construction year was recorded or not\n",
    "X_train['construction_year_recorded'] = np.where(X_train.construction_year == 0, False, True)\n",
    "X_test['construction_year_recorded'] = np.where(X_test.construction_year == 0, False, True)\n",
    "\n",
    "# replace construction_year == 0 with the mean construction year\n",
    "mean_construction_year = round(X_train.loc[X_train.construction_year != 0, 'construction_year'].mean(), 0)\n",
    "X_train.loc[X_train.construction_year == 0, 'construction_year'] = mean_construction_year\n",
    "X_test.loc[X_test.construction_year == 0, 'construction_year'] = mean_construction_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:48.157442Z",
     "start_time": "2020-03-17T22:46:48.107263Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a column storing the info whether longitude/latitude was recorded or not\n",
    "X_train['longitude_recorded'] = np.where(abs(X_train.longitude) < 0.1, False, True)\n",
    "X_train['latitude_recorded'] = np.where(abs(X_train.latitude) < 0.1, False, True)\n",
    "\n",
    "X_test['longitude_recorded'] = np.where(X_test.longitude < 0.1, False, True)\n",
    "X_test['latitude_recorded'] = np.where(X_test.latitude < 0.1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:48.318549Z",
     "start_time": "2020-03-17T22:46:48.240652Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace missing values in public_meeting with the majority category (True)\n",
    "X_train.loc[X_train.public_meeting.isna(), 'public_meeting'] = True\n",
    "X_test.loc[X_test.public_meeting.isna(), 'public_meeting'] = True\n",
    "\n",
    "# replace missing values in permit with the majority category (True)\n",
    "X_train.loc[X_train.permit.isna(), 'permit'] = True\n",
    "X_test.loc[X_test.permit.isna(), 'permit'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. One hot encoding and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:52.013235Z",
     "start_time": "2020-03-17T22:46:50.416172Z"
    }
   },
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "X_train = pd.get_dummies(X_train, \n",
    "                         prefix = X_train.select_dtypes('object').columns, \n",
    "                         columns = X_train.select_dtypes('object').columns,\n",
    "                         drop_first = True\n",
    "                        )\n",
    "\n",
    "X_test = pd.get_dummies(X_test, \n",
    "                         prefix = X_test.select_dtypes('object').columns, \n",
    "                         columns = X_test.select_dtypes('object').columns,\n",
    "                         drop_first = True\n",
    "                        )\n",
    "\n",
    "# power transformation of numerical columns\n",
    "numerical_columns = X_train.select_dtypes(['int64', 'float64']).columns\n",
    "\n",
    "pt = PowerTransformer()\n",
    "X_train.loc[:,numerical_columns] = pt.fit_transform(X_train.loc[:,numerical_columns])\n",
    "X_test.loc[:,numerical_columns] = pt.transform(X_test.loc[:,numerical_columns])\n",
    "\n",
    "# add columns to test set that only exist in train set\n",
    "X_test[list(set(X_train.columns).difference(set(X_test.columns)))[0]] = 0\n",
    "\n",
    "# make sure columns are in the same order\n",
    "X_train = X_train[sorted(X_train.columns)].copy()\n",
    "X_test = X_test[sorted(X_test.columns)].copy()\n",
    "\n",
    "\n",
    "# creating a mapping for the classes\n",
    "classes = {\n",
    "    'functional' : 0,\n",
    "    'non functional' : 1,\n",
    "    'functional needs repair' : 2\n",
    "}\n",
    "\n",
    "# create the inverse mapping\n",
    "classes_inv = {v: k for k, v in classes.items()}\n",
    "\n",
    "# map the target to numerical\n",
    "y_train = y_train.status_group.map(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Feature Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 K-Means Clustering For Latitude and Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach we wanted to better capture different regions best on their geographical location instead of having the raw longitude and latitude coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:52.280823Z",
     "start_time": "2020-03-17T22:46:52.257328Z"
    }
   },
   "outputs": [],
   "source": [
    "#extracting only latitude and longitude from the features\n",
    "\n",
    "X_train_geo= X_train[['latitude','longitude']]\n",
    "X_test_geo= X_test[['latitude','longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:59.029841Z",
     "start_time": "2020-03-17T22:46:52.550021Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the clusters. We tried 9 clusters based on the number of basins found in the EDA\n",
    "\n",
    "kmeans = KMeans(n_clusters = 9, init ='k-means++')\n",
    "kmeans.fit(X_train_geo[X_train_geo.columns[0:2]]) \n",
    "X_train_geo['cluster_label'] = kmeans.fit_predict(X_train_geo[X_train_geo.columns[0:2]])\n",
    "\n",
    "kmeans = KMeans(n_clusters = 9, init ='k-means++')\n",
    "kmeans.fit(X_test_geo[X_test_geo.columns[0:2]])\n",
    "X_test_geo['cluster_label'] = kmeans.fit_predict(X_test_geo[X_test_geo.columns[0:2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:59.282128Z",
     "start_time": "2020-03-17T22:46:59.034275Z"
    }
   },
   "outputs": [],
   "source": [
    "#dropping the longitude and latitude columns  from the features\n",
    "columns_to_drop = [\"longitude\",\"latitude\"]\n",
    "X_train.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "X_test.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "#creating DataFrames with the new features\n",
    "X_train_geo = pd.get_dummies(X_train_geo[\"cluster_label\"], drop_first=True)\n",
    "X_test_geo = pd.get_dummies(X_test_geo[\"cluster_label\"], drop_first=True)\n",
    "\n",
    "#concatenating them with the original features DF\n",
    "X_train = pd.concat([X_train,X_train_geo], axis = 1)\n",
    "X_test = pd.concat([X_test,X_test_geo], axis = 1)\n",
    "\n",
    "#renaming the columns\n",
    "X_train= X_train.rename(columns={1: \"cluster_1\",2:\"cluster_2\",3:\"cluster_3\",4:\"cluster_4\", 5:\"cluster_5\",6:\"cluster_6\",7:\"cluster_7\", 8: \"cluster_8\"})\n",
    "X_test= X_test.rename(columns={1: \"cluster_1\",2:\"cluster_2\",3:\"cluster_3\",4:\"cluster_4\", 5:\"cluster_5\",6:\"cluster_6\",7:\"cluster_7\", 8: \"cluster_8\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:59.322291Z",
     "start_time": "2020-03-17T22:46:59.310979Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test.index = range(len(X_train),(len(X_train) + len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:46:59.416299Z",
     "start_time": "2020-03-17T22:46:59.329441Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X_train,X_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Deep Feature Synthesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the same variables are created (we saw there was an issue when we split DFS into train and test), we are going to join train and test feature for the automated feature creation to later re-join them by the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure the index in X_test starts after X_train begins for the join\n",
    "X_test.index = range(len(X_train),(len(X_train) + len(X_test)))\n",
    "#join the two dfs\n",
    "X = pd.concat([X_train,X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:47:10.741667Z",
     "start_time": "2020-03-17T22:47:10.696003Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#defining my entity set\n",
    "es = ft.EntitySet(id = 'id')\n",
    "\n",
    "#defining boolean varialbes as boolean following approach by https://brendanhasz.github.io/2018/11/11/featuretools\n",
    "BOOL = ft.variable_types.Boolean\n",
    "\n",
    "#variable types dictionary for boolean variables not to be identified as numerical\n",
    "variable_types = {\n",
    "    'cluster_1':BOOL,\n",
    "    'cluster_2':BOOL,\n",
    "    'cluster_3':BOOL,\n",
    "    'cluster_4':BOOL,\n",
    "    'cluster_5':BOOL,\n",
    "    'cluster_6':BOOL,\n",
    "    'cluster_7':BOOL,\n",
    "    'cluster_8':BOOL,\n",
    "    'basin_Lake Nyasa': BOOL,\n",
    "    'basin_Lake Rukwa': BOOL,\n",
    "    'basin_Lake Tanganyika': BOOL,\n",
    "    'basin_Lake Victoria': BOOL,\n",
    "    'basin_Pangani': BOOL,\n",
    "    'basin_Rufiji': BOOL,\n",
    "    'basin_Ruvuma / Southern Coast': BOOL,\n",
    "    'basin_Wami / Ruvu': BOOL,\n",
    "    'extraction_type_cemo': BOOL,\n",
    "    'extraction_type_climax': BOOL,\n",
    "    'extraction_type_gravity': BOOL,\n",
    "    'extraction_type_india mark ii': BOOL,\n",
    "    'extraction_type_india mark iii': BOOL,\n",
    "    'extraction_type_ksb': BOOL,\n",
    "    'extraction_type_mono': BOOL,\n",
    "    'extraction_type_nira/tanira': BOOL,\n",
    "    'extraction_type_other': BOOL,\n",
    "    'extraction_type_other - mkulima/shinyanga': BOOL,\n",
    "    'extraction_type_other - play pump': BOOL,\n",
    "    'extraction_type_other - rope pump': BOOL,\n",
    "    'extraction_type_other - swn 81': BOOL,\n",
    "    'extraction_type_submersible': BOOL,\n",
    "    'extraction_type_swn 80': BOOL,\n",
    "    'extraction_type_walimi': BOOL,\n",
    "    'extraction_type_windmill': BOOL,\n",
    "    'management_other': BOOL,\n",
    "    'management_other - school': BOOL,\n",
    "    'management_parastatal': BOOL,\n",
    "    'management_private operator': BOOL,\n",
    "    'management_trust': BOOL,\n",
    "    'management_unknown': BOOL,\n",
    "    'management_vwc': BOOL,\n",
    "    'management_water authority': BOOL,\n",
    "    'management_water board': BOOL,\n",
    "    'management_wua': BOOL,\n",
    "    'management_wug': BOOL,\n",
    "    'payment_type_monthly': BOOL,\n",
    "    'payment_type_never pay': BOOL,\n",
    "    'payment_type_on failure': BOOL,\n",
    "    'payment_type_other': BOOL,\n",
    "    'payment_type_per bucket': BOOL,\n",
    "    'payment_type_unknown': BOOL,\n",
    "    'permit_True': BOOL,\n",
    "    'public_meeting_True': BOOL,\n",
    "    'quantity_enough': BOOL,\n",
    "    'quantity_insufficient': BOOL,\n",
    "    'quantity_seasonal': BOOL,\n",
    "    'quantity_unknown': BOOL,\n",
    "    'region_Dar es Salaam': BOOL,\n",
    "    'region_Dodoma': BOOL,\n",
    "    'region_Iringa': BOOL,\n",
    "    'region_Kagera': BOOL,\n",
    "    'region_Kigoma': BOOL,\n",
    "    'region_Kilimanjaro': BOOL,\n",
    "    'region_Lindi': BOOL,\n",
    "    'region_Manyara': BOOL,\n",
    "    'region_Mara': BOOL,\n",
    "    'region_Mbeya': BOOL,\n",
    "    'region_Morogoro': BOOL,\n",
    "    'region_Mtwara': BOOL,\n",
    "    'region_Mwanza': BOOL,\n",
    "    'region_Pwani': BOOL,\n",
    "    'region_Rukwa': BOOL,\n",
    "    'region_Ruvuma': BOOL,\n",
    "    'region_Shinyanga': BOOL,\n",
    "    'region_Singida': BOOL,\n",
    "    'region_Tabora': BOOL,\n",
    "    'region_Tanga': BOOL,\n",
    "    'source_hand dtw': BOOL,\n",
    "    'source_lake': BOOL,\n",
    "    'source_machine dbh': BOOL,\n",
    "    'source_other': BOOL,\n",
    "    'source_rainwater harvesting': BOOL,\n",
    "    'source_river': BOOL,\n",
    "    'source_shallow well': BOOL,\n",
    "    'source_spring': BOOL,\n",
    "    'source_unknown': BOOL,\n",
    "    'water_quality_fluoride': BOOL,\n",
    "    'water_quality_fluoride abandoned': BOOL,\n",
    "    'water_quality_milky': BOOL,\n",
    "    'water_quality_salty': BOOL,\n",
    "    'water_quality_salty abandoned': BOOL,\n",
    "    'water_quality_soft': BOOL,\n",
    "    'water_quality_unknown': BOOL,\n",
    "    'waterpoint_type_communal standpipe': BOOL,\n",
    "    'waterpoint_type_communal standpipe multiple': BOOL,\n",
    "    'waterpoint_type_dam': BOOL,\n",
    "    'waterpoint_type_hand pump': BOOL,\n",
    "    'waterpoint_type_improved spring': BOOL,\n",
    "    'waterpoint_type_other': BOOL\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:47:13.159333Z",
     "start_time": "2020-03-17T22:47:13.016743Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating my entities for X_train and X_test with the defined variable types\n",
    "\n",
    "es = es.entity_from_dataframe(entity_id = 'entity_id', dataframe = X, \n",
    "                              index = 'id', variable_types=variable_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:48:58.326200Z",
     "start_time": "2020-03-17T22:47:18.992303Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating the new features using dfs algorithm. \n",
    "#For numerical values we chose the mean, skewness, max value and std as aggregators and multiply_numeric and divide_numeric as transformations\n",
    "#For boolean variables, we selected \"and\" as the only transformation variable (if 2 booleans are true at the same time)\n",
    "\n",
    "features, feature_names = ft.dfs(entityset=es, target_entity='entity_id',agg_primitives =[\"mean\",\"skew\",\"max\",\"std\"]\n",
    "                        ,trans_primitives = [\"and\",\"multiply_numeric\",\"divide_numeric\"], max_depth = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T22:48:58.334464Z",
     "start_time": "2020-03-17T22:48:58.328054Z"
    }
   },
   "outputs": [],
   "source": [
    "#set the index for the created features to split again after\n",
    "features.index = (range(0,(len(X_train) + len(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. PCA for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As DFS left us with over 4K features, we are going to perform PCA to reduce them into 50 components, which is around half of the original number of features. As such we expect them to capture the principal components of the DFS, as inputting the 4k variables is over our computer processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T23:23:20.742241Z",
     "start_time": "2020-03-17T23:22:09.469162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=50, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first we redefine X_train and X_test as the features we created\n",
    "X_train_dfs = features.loc[0:59399,:]\n",
    "X_test_dfs = features.loc[59400:74250,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T23:24:02.358186Z",
     "start_time": "2020-03-17T23:23:20.749872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=50, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform the PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(X_train_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T23:24:22.541596Z",
     "start_time": "2020-03-17T23:24:02.366335Z"
    }
   },
   "outputs": [],
   "source": [
    "#transform our features\n",
    "X_train_dfs = pca.transform(X_train_dfs)\n",
    "X_test_dfs = pca.transform(X_test_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model 1 - KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input the new train variables and slightly tune the number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T23:30:44.418417Z",
     "start_time": "2020-03-17T23:24:22.543595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'n_neighbors': 9}</td>\n",
       "      <td>0.744394</td>\n",
       "      <td>0.782177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'n_neighbors': 7}</td>\n",
       "      <td>0.744343</td>\n",
       "      <td>0.791360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'n_neighbors': 11}</td>\n",
       "      <td>0.743232</td>\n",
       "      <td>0.774654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                params  mean_test_score  mean_train_score\n",
       "1   {'n_neighbors': 9}         0.744394          0.782177\n",
       "0   {'n_neighbors': 7}         0.744343          0.791360\n",
       "2  {'n_neighbors': 11}         0.743232          0.774654"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# create a knn classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# create a param grid\n",
    "param_grid = {'n_neighbors' : [7,9,11]}\n",
    "\n",
    "# do a grid search to find the best parameter\n",
    "grid_knn = GridSearchCV(\n",
    "    estimator = knn,\n",
    "    param_grid = param_grid,\n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = -1,\n",
    "    cv = 10,\n",
    "    refit = True,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "grid_knn.fit(X_train_dfs, y_train)\n",
    "\n",
    "# read results of grid search into dataframe\n",
    "cv_results_df = pd.DataFrame(grid_knn.cv_results_)\n",
    "\n",
    "# print results\n",
    "cv_results_df[['params', 'mean_test_score', 'mean_train_score']].sort_values(by = ['mean_test_score'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Model : XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to give it a try, we decided to put the two PCA components in to an XGBoost model, as we got the highest scores from it in all our efforts. Fitting this model took a lot of computer power and almost 40 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T23:30:44.433755Z",
     "start_time": "2020-03-17T23:30:44.421822Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "param_test = {\n",
    " 'max_depth':[7,8,9],\n",
    " 'min_child_weight':[2,3],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T23:30:44.449247Z",
     "start_time": "2020-03-17T23:30:44.436211Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#model creation and gridsearch\n",
    "xgb_model = xgb.XGBClassifier(learning_rate=0.1, \n",
    "                              n_estimators=120, \n",
    "                              gamma=0.2, \n",
    "                              #max_depth = 14,\n",
    "                              #min_child_weight = 2,\n",
    "                              num_class = 3,\n",
    "                              subsample=0.8, \n",
    "                              colsample_bytree=0.8,\n",
    "                              objective= 'multi:softmax', \n",
    "                              nthread=4, \n",
    "                              scale_pos_weight=1,\n",
    "                              seed=27)\n",
    "\n",
    "gsearch = GridSearchCV(estimator = xgb_model, \n",
    "                       param_grid = param_test, \n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=4,\n",
    "                       cv=5,\n",
    "                       refit = True,\n",
    "                       return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T00:10:33.160692Z",
     "start_time": "2020-03-17T23:30:44.451998Z"
    }
   },
   "outputs": [],
   "source": [
    "train_model_9 = gsearch.fit(X_train_dfs, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T00:10:33.191724Z",
     "start_time": "2020-03-18T00:10:33.163457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 7, 'min_child_weight': 2}</td>\n",
       "      <td>0.777458</td>\n",
       "      <td>0.835968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 7, 'min_child_weight': 3}</td>\n",
       "      <td>0.777761</td>\n",
       "      <td>0.834276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 8, 'min_child_weight': 2}</td>\n",
       "      <td>0.782071</td>\n",
       "      <td>0.860408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 8, 'min_child_weight': 3}</td>\n",
       "      <td>0.781633</td>\n",
       "      <td>0.856688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_depth': 9, 'min_child_weight': 2}</td>\n",
       "      <td>0.783468</td>\n",
       "      <td>0.882727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'max_depth': 9, 'min_child_weight': 3}</td>\n",
       "      <td>0.783603</td>\n",
       "      <td>0.878081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    params  mean_test_score  mean_train_score\n",
       "0  {'max_depth': 7, 'min_child_weight': 2}         0.777458          0.835968\n",
       "1  {'max_depth': 7, 'min_child_weight': 3}         0.777761          0.834276\n",
       "2  {'max_depth': 8, 'min_child_weight': 2}         0.782071          0.860408\n",
       "3  {'max_depth': 8, 'min_child_weight': 3}         0.781633          0.856688\n",
       "4  {'max_depth': 9, 'min_child_weight': 2}         0.783468          0.882727\n",
       "5  {'max_depth': 9, 'min_child_weight': 3}         0.783603          0.878081"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gsearch.cv_results_)[['params', 'mean_test_score', 'mean_train_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores look slightly higher than KNN but the model looks overfitted. However the gap between test and train scores is not as big as when we did multiple iterations of hyperparameter tuning on XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Prediction for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T00:13:43.501554Z",
     "start_time": "2020-03-18T00:13:30.409550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8920\n",
       "1    5479\n",
       "2     451\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#predicting y with the best parameters and checking the output makes sense\n",
    "\n",
    "y_pred = grid_knn.best_estimator_.predict(X_test_dfs)\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "\n",
    "#to make sure distribution of classes make sense\n",
    "y_pred_df[0].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T23:19:28.058795Z",
     "start_time": "2020-03-17T23:19:28.006734Z"
    }
   },
   "outputs": [],
   "source": [
    "# map back to string classes\n",
    "y_pred = pd.Series(y_pred).map(classes_inv)\n",
    "\n",
    "# create submission data frame\n",
    "y_test.loc[:,'status_group'] = y_pred\n",
    "\n",
    "# write to csv\n",
    "y_test.to_csv('submission11_dfs_pca_knn.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Prediction for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T00:15:01.435064Z",
     "start_time": "2020-03-18T00:15:01.133279Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred= gsearch.best_estimator_.predict(X_test_dfs)\n",
    "y_pred_df= pd.DataFrame(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T00:15:02.122523Z",
     "start_time": "2020-03-18T00:15:02.113987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8747\n",
       "1    6003\n",
       "2     100\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to make sure the values make sense.\n",
    "y_pred_df[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T00:15:16.162950Z",
     "start_time": "2020-03-18T00:15:16.111655Z"
    }
   },
   "outputs": [],
   "source": [
    "# map back to string classes\n",
    "y_pred = pd.Series(y_pred).map(classes_inv)\n",
    "\n",
    "# create submission data frame\n",
    "y_test.loc[:,'status_group'] = y_pred\n",
    "\n",
    "# write to csv\n",
    "y_test.to_csv('submission11_dfs_pca_xgboost.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
